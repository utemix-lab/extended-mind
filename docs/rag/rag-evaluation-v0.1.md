---
title: RAG Evaluation — Extended-mind v0.1
version: 0.1
type: process
status: draft-core
tags: [rag, evaluation, quality]
lastmod: 2025-12-13
description: "Минимальный процесс оценки качества RAG в extended-mind: набор вопросов, ручная проверка, простые критерии."
---

# RAG Evaluation — Extended-mind v0.1

---

## 1. Зачем нужен eval на старте

Extended-mind должен быть **учителем**, а не попугаем:

- не просто “выплёвывать текст”,
- а реально опираться на базу знаний,
- не забывать принятые решения.

Eval v0.1 — это **ручная, но структурированная проверка**:  
набор вопросов + критерии, чтобы отличить:

- “хороший ответ на основе RAG”  
от
- “сочинение на свободную тему”.

---

## 2. Типы тестовых вопросов

Формируем **минимальный тестовый набор** (10–20 вопросов) по типам:

1. **Определения**  
   - “Что такое story-layer в extended-mind?”
   - “Кто такой author-line?”
   - “Что такое thin-tank в контексте V&P?”

2. **Решения (ADR-уровень)**  
   - “Почему мы решили начинать с RAG, а не с графа?”
   - “Почему extended-mind говорит от первого лица?”
   - “Что нельзя автоматизировать в extended-mind?”

3. **Паттерны / процессы**  
   - “Как превращать story-node в тезисы?”
   - “Какую роль играют patterns в extended-mind?”

4. **Проекты**  
   - “Где проходит граница между extended-mind и проектом «Вова и Петрова»?”
   - “Что делает thin-tank для V&P?”

На старте можно просто хранить этот список в `docs/rag/eval-questions-v0.1.md`.

---

## 3. Процедура проверки (ручной eval)

1. Запустить RAG-обвязку (чтобы extended-mind отвечал с использованием индекса).
2. Задать по очереди все тестовые вопросы.
3. Для каждого ответа:
   - сохранить текст ответа,
   - (по возможности) сохранить чанки, которые вернул retrieval.

4. Оценить ответы **вручную** по критериям v0.1 (см. ниже).

---

## 4. Критерии качества v0.1

Каждый ответ оценивается по шкале **0–2**:

- **2 — Хорошо:**
  - ответ явно основан на документах extended-mind,
  - использует терминологию и формулировки, близкие к README/ADR/definitions,
  - не противоречит принятым решениям,
  - даёт короткое, но точное объяснение.

- **1 — Так себе:**
  - ответ формально похож на правду,
  - но:
    - слишком общий,
    - не использует “свою” лексику (extended-mind),
    - или не даёт нужного уровня точности.

- **0 — Плохо:**
  - явное противоречие документам,
  - выдумывание несуществующих решений/определений,
  - игнорирование ключевых понятий.

Можно завести таблицу (даже в markdown):

```markdown
| # | Вопрос                                          | Оценка (0–2) | Комментарий |
|---|-------------------------------------------------|--------------|-------------|
| 1 | Что такое story-layer?                          | 2            |             |
| 2 | Почему RAG раньше графа?                        | 1            | слишком общий|
| 3 | Кто такой author-line?                          | 2            |             |
| ... | ...                                           | ...          | ...         |
5. Порог “нормального состояния”
Для v0.1 считаем, что RAG:

“Годен к использованию для самообразования”,

если:

средняя оценка по всем вопросам ≥ 1.5,

ни один критический вопрос (про ключевые сущности)
не получил 0.

Критические вопросы можно явно отметить в списке (например, пометкой critical).

6. Типичные проблемы и как на них реагировать
6.1. Ответы слишком общие
Вероятные причины:

слишком крупные чанки,

мало контекста в индексированных документах,

нерелевантные чанки в top-k.

Что делать:

уменьшить размер чанка или увеличить пересечение,

пересмотреть, какие документы входят в индекс,

временно уменьшить top_k (чтобы не тащить всё подряд).

6.2. Ответы противоречат ADR
Причины:

модель “додумывает”,

retrieval не нашёл нужный ADR.

Что делать:

проверить, попал ли нужный ADR в индекс,

проверить, действительно ли ADR описывает вопрос явно,

при необходимости:

усилить формулировки ADR,

добавить отдельные “definition” по этому вопросу.

6.3. Ответы “не в стиле extended-mind”
Причины:

system-prompt extended-mind не подгружен в RAG-контекст,

модель не видит “правильный голос”.

Что делать:

гарантировать, что docs/core/system-prompt.md входит в индекс,

иногда явно подмешивать выдержки из него в prompt.

7. Переход к v0.2 eval
В будущем можно:

добавить полуавтоматический eval:

хранить пары “вопрос → эталонный ответ в 2–3 предложениях”,

сравнивать ответы модели с эталоном по эмбеддингам или LLM-проверкой.

Но для старта v0.1 достаточно ручной, осмысленной проверки,
которая сама по себе является частью обучения автора и оттачивания extended-mind.

8. Статус
Этот документ — минимальная, но живая методика.
Цель — не сделать научный бенчмарк,
а получить инструмент здравого смысла:

“Extended-mind помнит, как мы договорились мыслить,
и не отрывается от своих же документов”.
